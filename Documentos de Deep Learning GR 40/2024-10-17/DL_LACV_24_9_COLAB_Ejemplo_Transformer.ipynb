{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, datasets, models\n",
        "import numpy as np\n",
        "\n",
        "# Cargar el conjunto de datos CIFAR-10\n",
        "(x_train, y_train), (x_test, y_test) = datasets.cifar10.load_data()\n",
        "\n",
        "# Normalizar los datos de entrada\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "# Convertir etiquetas a one-hot encoding\n",
        "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "# Parámetros del transformer\n",
        "PATCH_SIZE = 4  # Dividimos la imagen en patches de 4x4\n",
        "NUM_PATCHES = (32 // PATCH_SIZE) ** 2  # CIFAR10 tiene imágenes de 32x32\n",
        "D_MODEL = 64  # Tamaño de la representación por patch\n",
        "NUM_HEADS = 8  # Número de cabezas para la atención múltiple\n",
        "D_FF = 128  # Tamaño del feedforward en el bloque transformer\n",
        "NUM_BLOCKS = 4  # Número de bloques transformer\n",
        "\n",
        "# Función para dividir las imágenes en patches\n",
        "def extract_patches(images, patch_size):\n",
        "    batch_size = tf.shape(images)[0]\n",
        "    patches = tf.image.extract_patches(\n",
        "        images=images,\n",
        "        sizes=[1, patch_size, patch_size, 1],\n",
        "        strides=[1, patch_size, patch_size, 1],\n",
        "        rates=[1, 1, 1, 1],\n",
        "        padding='VALID'\n",
        "    )\n",
        "    patch_dims = patches.shape[-1]\n",
        "    patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
        "    return patches\n",
        "\n",
        "# Implementar Self-Attention\n",
        "class MultiHeadSelfAttention(layers.Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadSelfAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "\n",
        "        assert d_model % self.num_heads == 0\n",
        "\n",
        "        self.depth = d_model // self.num_heads\n",
        "        self.wq = layers.Dense(d_model)\n",
        "        self.wk = layers.Dense(d_model)\n",
        "        self.wv = layers.Dense(d_model)\n",
        "        self.dense = layers.Dense(d_model)\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        \"\"\"Split the last dimension into (num_heads, depth).\"\"\"\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, v, k, q):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "\n",
        "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
        "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
        "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
        "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
        "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
        "\n",
        "        # Escalado de los valores\n",
        "        matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
        "        dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "        scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "        # Softmax para obtener las atenciones\n",
        "        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
        "\n",
        "        # Multiplicar las atenciones por los valores\n",
        "        output = tf.matmul(attention_weights, v)  # (batch_size, num_heads, seq_len_q, depth_v)\n",
        "\n",
        "        output = tf.transpose(output, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
        "        concat_attention = tf.reshape(output, (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
        "        return output\n",
        "\n",
        "# Bloque transformer básico\n",
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = MultiHeadSelfAttention(d_model, num_heads)\n",
        "        self.ffn = models.Sequential([\n",
        "            layers.Dense(dff, activation='relu'),  # Primera capa feedforward\n",
        "            layers.Dense(d_model)  # Segunda capa feedforward\n",
        "        ])\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(0.1)\n",
        "        self.dropout2 = layers.Dropout(0.1)\n",
        "\n",
        "    def call(self, x, training):\n",
        "        attn_output = self.att(x, x, x)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(x + attn_output)\n",
        "\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "# Modelo completo con patches y bloques transformer\n",
        "class TransformerClassifier(models.Model):\n",
        "    def __init__(self, num_patches, d_model, num_heads, dff, num_blocks, num_classes):\n",
        "        super(TransformerClassifier, self).__init__()\n",
        "        self.patch_proj = layers.Dense(d_model)\n",
        "        self.transformer_blocks = [TransformerBlock(d_model, num_heads, dff) for _ in range(num_blocks)]\n",
        "        self.pool = layers.GlobalAveragePooling1D()  # Agregamos un GlobalAveragePooling\n",
        "        self.fc = layers.Dense(num_classes, activation='softmax')\n",
        "\n",
        "    def call(self, x, training):\n",
        "        # Dividir en patches\n",
        "        patches = extract_patches(x, PATCH_SIZE)\n",
        "        x = self.patch_proj(patches)\n",
        "\n",
        "        # Aplicar bloques Transformer\n",
        "        for block in self.transformer_blocks:\n",
        "            x = block(x, training=training)\n",
        "\n",
        "        # Global average pooling para aplanar las dimensiones\n",
        "        x = self.pool(x)\n",
        "\n",
        "        # Clasificación final\n",
        "        return self.fc(x)\n",
        "\n",
        "# Parámetros del modelo\n",
        "num_classes = 10\n",
        "model = TransformerClassifier(NUM_PATCHES, D_MODEL, NUM_HEADS, D_FF, NUM_BLOCKS, num_classes)\n",
        "\n",
        "# Compilar el modelo\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Entrenamiento\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=64, validation_data=(x_test, y_test))\n",
        "\n",
        "# Evaluar el modelo en el conjunto de test\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(f'Precisión en test: {test_acc}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JDsdh2_HlL4p",
        "outputId": "6ce746b4-2f62-492a-9911-7828cbf5c16b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m316s\u001b[0m 381ms/step - accuracy: 0.2338 - loss: 2.0214 - val_accuracy: 0.3529 - val_loss: 1.7505\n",
            "Epoch 2/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m329s\u001b[0m 390ms/step - accuracy: 0.3973 - loss: 1.6204 - val_accuracy: 0.4607 - val_loss: 1.4781\n",
            "Epoch 3/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m298s\u001b[0m 381ms/step - accuracy: 0.4670 - loss: 1.4599 - val_accuracy: 0.4891 - val_loss: 1.3974\n",
            "Epoch 4/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m320s\u001b[0m 378ms/step - accuracy: 0.5027 - loss: 1.3649 - val_accuracy: 0.4966 - val_loss: 1.3851\n",
            "Epoch 5/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m322s\u001b[0m 379ms/step - accuracy: 0.5306 - loss: 1.2939 - val_accuracy: 0.5152 - val_loss: 1.3495\n",
            "Epoch 6/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m321s\u001b[0m 378ms/step - accuracy: 0.5464 - loss: 1.2577 - val_accuracy: 0.5475 - val_loss: 1.2708\n",
            "Epoch 7/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m325s\u001b[0m 382ms/step - accuracy: 0.5621 - loss: 1.2082 - val_accuracy: 0.5571 - val_loss: 1.2319\n",
            "Epoch 8/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m320s\u001b[0m 380ms/step - accuracy: 0.5784 - loss: 1.1698 - val_accuracy: 0.5673 - val_loss: 1.2073\n",
            "Epoch 9/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m297s\u001b[0m 379ms/step - accuracy: 0.5931 - loss: 1.1364 - val_accuracy: 0.5591 - val_loss: 1.2129\n",
            "Epoch 10/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m323s\u001b[0m 381ms/step - accuracy: 0.5969 - loss: 1.1133 - val_accuracy: 0.5622 - val_loss: 1.2195\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 65ms/step - accuracy: 0.5615 - loss: 1.2248\n",
            "Precisión en test: 0.5622000098228455\n"
          ]
        }
      ]
    }
  ]
}