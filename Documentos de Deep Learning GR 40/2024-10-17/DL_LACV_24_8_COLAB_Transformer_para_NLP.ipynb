{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m9JJ7FBw84tG"
   },
   "source": [
    "# Fase 1: Importar las dependencias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t5DbIHC-F6Hf"
   },
   "source": [
    "**Paper original**: All you need is Attention https://arxiv.org/pdf/1706.03762.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "ZbcvtPlp3YWu"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "\n",
    "\n",
    "# from google.colab import drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "P6o_cpZz3y_-"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    %tensorflow_version 2.x\n",
    "except:\n",
    "    pass\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BQN8jwx48_yU"
   },
   "source": [
    "# Fase 2: Pre Procesado de Datos\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bPlOT-2mlw0r"
   },
   "source": [
    "## Carga de Ficheros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dCD9jwXsLwS_"
   },
   "source": [
    "Importamos los ficheros de nuestro Google Drive personal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "id": "eQpbl1pXCR0p",
    "outputId": "1b754e89-8ee9-4b39-951e-ca4964cda8e5"
   },
   "outputs": [],
   "source": [
    "#  drive.mount(\"/content/drive\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "q8Or0sLV5b8t"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nwith open(\"/content/drive/My Drive/Curso de NLP/Transformer/data/europarl-v7.es-en.en\", \\n          mode = \"r\", encoding = \"utf-8\") as f:\\n    europarl_en = f.read()\\nwith open(\"/content/drive/My Drive/Curso de NLP/Transformer/data/europarl-v7.es-en.es\", \\n          mode = \"r\", encoding = \"utf-8\") as f:\\n    europarl_es = f.read()\\nwith open(\"/content/drive/My Drive/Curso de NLP/Transformer/data/nonbreaking_prefix.en\", \\n          mode = \"r\", encoding = \"utf-8\") as f:\\n    non_breaking_prefix_en = f.read()\\nwith open(\"/content/drive/My Drive/Curso de NLP/Transformer/data/nonbreaking_prefix.es\", \\n          mode = \"r\", encoding = \"utf-8\") as f:\\n    non_breaking_prefix_es = f.read()\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "with open(\"/content/drive/My Drive/Curso de NLP/Transformer/data/europarl-v7.es-en.en\", \n",
    "          mode = \"r\", encoding = \"utf-8\") as f:\n",
    "    europarl_en = f.read()\n",
    "with open(\"/content/drive/My Drive/Curso de NLP/Transformer/data/europarl-v7.es-en.es\", \n",
    "          mode = \"r\", encoding = \"utf-8\") as f:\n",
    "    europarl_es = f.read()\n",
    "with open(\"/content/drive/My Drive/Curso de NLP/Transformer/data/nonbreaking_prefix.en\", \n",
    "          mode = \"r\", encoding = \"utf-8\") as f:\n",
    "    non_breaking_prefix_en = f.read()\n",
    "with open(\"/content/drive/My Drive/Curso de NLP/Transformer/data/nonbreaking_prefix.es\", \n",
    "          mode = \"r\", encoding = \"utf-8\") as f:\n",
    "    non_breaking_prefix_es = f.read()\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"europarl-v7.es-en.en\", \n",
    "          mode = \"r\", encoding = \"utf-8\") as f:\n",
    "    europarl_en = f.read()\n",
    "with open(\"europarl-v7.es-en.es\", \n",
    "          mode = \"r\", encoding = \"utf-8\") as f:\n",
    "    europarl_es = f.read()\n",
    "with open(\"nonbreaking_prefix.en\", \n",
    "          mode = \"r\", encoding = \"utf-8\") as f:\n",
    "    non_breaking_prefix_en = f.read()\n",
    "with open(\"nonbreaking_prefix.es\", \n",
    "          mode = \"r\", encoding = \"utf-8\") as f:\n",
    "    non_breaking_prefix_es = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 58
    },
    "id": "TMAFFdpIyNZd",
    "outputId": "eb684c33-6895-4fc9-a27d-420c05a5fab8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Resumption of the session\\nI declare resumed the session of the European Parliament adjourned on Frid'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "europarl_en[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 58
    },
    "id": "BYgCMq6myYIi",
    "outputId": "f76b75fd-25df-43f2-fe4b-f88de485e69d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Reanudación del período de sesiones\\nDeclaro reanudado el período de sesiones del Parlamento Europeo,'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "europarl_es[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TEFw0D2vP_Dl"
   },
   "source": [
    "## Limpieza de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PwIBeGXn7LIJ"
   },
   "source": [
    "Vamos a obtener los non_breaking_prefixes como una lista de palabras limpias con un punto al final para que nos sea más fácil de utilizar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "L_TeuktU40Cb"
   },
   "outputs": [],
   "source": [
    "non_breaking_prefix_en = non_breaking_prefix_en.split(\"\\n\")\n",
    "non_breaking_prefix_en = [' ' + pref + '.' for pref in non_breaking_prefix_en]\n",
    "non_breaking_prefix_es = non_breaking_prefix_es.split(\"\\n\")\n",
    "non_breaking_prefix_es = [' ' + pref + '.' for pref in non_breaking_prefix_es]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H9x4mZfKMaxD"
   },
   "source": [
    "Necesitaremos cada palabra y otro símbolo que queramos mantener en minúsculas y separados por espacios para que podamos \"tokenizarlos\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "Qg-8LLK-WdFp"
   },
   "outputs": [],
   "source": [
    "corpus_en = europarl_en\n",
    "# Añadimos $$$ después de los puntos de frases sin fin\n",
    "for prefix in non_breaking_prefix_en:\n",
    "    corpus_en = corpus_en.replace(prefix, prefix + '$$$')\n",
    "corpus_en = re.sub(r\"\\.(?=[0-9]|[a-z]|[A-Z])\", \".$$$\", corpus_en)\n",
    "# Eliminamos los marcadores $$$\n",
    "corpus_en = re.sub(r\"\\.\\$\\$\\$\", '', corpus_en)\n",
    "# Eliminamos espacios múltiples\n",
    "corpus_en = re.sub(r\"  +\", \" \", corpus_en)\n",
    "corpus_en = corpus_en.split('\\n')\n",
    "\n",
    "corpus_es = europarl_es\n",
    "for prefix in non_breaking_prefix_es:\n",
    "    corpus_es = corpus_es.replace(prefix, prefix + '$$$')\n",
    "corpus_es = re.sub(r\"\\.(?=[0-9]|[a-z]|[A-Z])\", \".$$$\", corpus_es)\n",
    "corpus_es = re.sub(r\"\\.\\$\\$\\$\", '', corpus_es)\n",
    "corpus_es = re.sub(r\"  +\", \" \", corpus_es)\n",
    "corpus_es = corpus_es.split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s-Y9v8-Tozl2"
   },
   "source": [
    "## Tokenizar el Texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "p5YXanmOd_xK"
   },
   "outputs": [],
   "source": [
    "tokenizer_en = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
    "    corpus_en, target_vocab_size=2**13)\n",
    "tokenizer_es = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
    "    corpus_es, target_vocab_size=2**13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "ftIbPzIwCtwL"
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE_EN = tokenizer_en.vocab_size + 2 # = 8198\n",
    "VOCAB_SIZE_ES = tokenizer_es.vocab_size + 2 # = 8225"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "oPFe2YJDC9jw"
   },
   "outputs": [],
   "source": [
    "inputs = [[VOCAB_SIZE_EN-2] + tokenizer_en.encode(sentence) + [VOCAB_SIZE_EN-1]\n",
    "          for sentence in corpus_en]\n",
    "outputs = [[VOCAB_SIZE_ES-2] + tokenizer_es.encode(sentence) + [VOCAB_SIZE_ES-1]\n",
    "           for sentence in corpus_es]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bG6AlcFMpC5C"
   },
   "source": [
    "## Eliminamos las frases demasiado largas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "F6CD6PLGyQWy"
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 20\n",
    "idx_to_remove = [count for count, sent in enumerate(inputs)\n",
    "                 if len(sent) > MAX_LENGTH]\n",
    "for idx in reversed(idx_to_remove):\n",
    "    del inputs[idx]\n",
    "    del outputs[idx]\n",
    "idx_to_remove = [count for count, sent in enumerate(outputs)\n",
    "                 if len(sent) > MAX_LENGTH]\n",
    "for idx in reversed(idx_to_remove):\n",
    "    del inputs[idx]\n",
    "    del outputs[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ypm8h5aZQTZ1"
   },
   "source": [
    "## Creamos las entradas y las salidas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9FP0WPsdM8hl"
   },
   "source": [
    "A medida que entrenamos con bloques, necesitaremos que cada entrada tenga la misma longitud. Rellenamos con el token apropiado, y nos aseguraremos de que este token de relleno no interfiera con nuestro entrenamiento más adelante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "nvDfLDWUONlE"
   },
   "outputs": [],
   "source": [
    "inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs,\n",
    "                                                       value=0,\n",
    "                                                       padding='post',\n",
    "                                                       maxlen=MAX_LENGTH)\n",
    "outputs = tf.keras.preprocessing.sequence.pad_sequences(outputs,\n",
    "                                                        value=0,\n",
    "                                                        padding='post',\n",
    "                                                        maxlen=MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "wFxMp3TOIYff"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 20000\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((inputs, outputs))\n",
    "\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ycT0YqydRcUd"
   },
   "source": [
    "# Fase 3: Construcción del Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-SBoH8G4XyR9"
   },
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7G9C3ucmJ86I"
   },
   "source": [
    "Fórmula de la Codificación Posicional:\n",
    "\n",
    "$PE_{(pos,2i)} =\\sin(pos/10000^{2i/dmodel})$\n",
    "\n",
    "$PE_{(pos,2i+1)} =\\cos(pos/10000^{2i/dmodel})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "e2wc6sYlX0dr"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(layers.Layer):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "    \n",
    "    def get_angles(self, pos, i, d_model): # pos: (seq_length, 1) i: (1, d_model)\n",
    "        angles = 1 / np.power(10000., (2*(i//2)) / np.float32(d_model))\n",
    "        return pos * angles # (seq_length, d_model)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        seq_length = inputs.shape.as_list()[-2]\n",
    "        d_model = inputs.shape.as_list()[-1]\n",
    "        angles = self.get_angles(np.arange(seq_length)[:, np.newaxis],\n",
    "                                 np.arange(d_model)[np.newaxis, :],\n",
    "                                 d_model)\n",
    "        angles[:, 0::2] = np.sin(angles[:, 0::2])\n",
    "        angles[:, 1::2] = np.cos(angles[:, 1::2])\n",
    "        pos_encoding = angles[np.newaxis, ...]\n",
    "        return inputs + tf.cast(pos_encoding, tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lcw8YIQqRhOJ"
   },
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3sffhwwvX-wj"
   },
   "source": [
    "### Cálculo de la Atención"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7VBuW6lESLDX"
   },
   "source": [
    "$Attention(Q, K, V ) = \\text{softmax}\\left(\\dfrac{QK^T}{\\sqrt{d_k}}\\right)V $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "2rEoCNJURbrT"
   },
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(queries, keys, values, mask):\n",
    "    product = tf.matmul(queries, keys, transpose_b=True)\n",
    "    \n",
    "    keys_dim = tf.cast(tf.shape(keys)[-1], tf.float32)\n",
    "    scaled_product = product / tf.math.sqrt(keys_dim)\n",
    "    \n",
    "    if mask is not None:\n",
    "        scaled_product += (mask * -1e9)\n",
    "    \n",
    "    attention = tf.matmul(tf.nn.softmax(scaled_product, axis=-1), values)\n",
    "    \n",
    "    return attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-MjtvXrfYEx7"
   },
   "source": [
    "### Sub capa de atención de encabezado múltiple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "lvq4I9uTX5p7"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(layers.Layer):\n",
    "    \n",
    "    def __init__(self, nb_proj):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.nb_proj = nb_proj\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.d_model = input_shape[-1]\n",
    "        assert self.d_model % self.nb_proj == 0\n",
    "        \n",
    "        self.d_proj = self.d_model // self.nb_proj\n",
    "        \n",
    "        self.query_lin = layers.Dense(units=self.d_model)\n",
    "        self.key_lin = layers.Dense(units=self.d_model)\n",
    "        self.value_lin = layers.Dense(units=self.d_model)\n",
    "        \n",
    "        self.final_lin = layers.Dense(units=self.d_model)\n",
    "        \n",
    "    def split_proj(self, inputs, batch_size): # inputs: (batch_size, seq_length, d_model)\n",
    "        shape = (batch_size,\n",
    "                 -1,\n",
    "                 self.nb_proj,\n",
    "                 self.d_proj)\n",
    "        splited_inputs = tf.reshape(inputs, shape=shape) # (batch_size, seq_length, nb_proj, d_proj)\n",
    "        return tf.transpose(splited_inputs, perm=[0, 2, 1, 3]) # (batch_size, nb_proj, seq_length, d_proj)\n",
    "    \n",
    "    def call(self, queries, keys, values, mask):\n",
    "        batch_size = tf.shape(queries)[0]\n",
    "        \n",
    "        queries = self.query_lin(queries)\n",
    "        keys = self.key_lin(keys)\n",
    "        values = self.value_lin(values)\n",
    "        \n",
    "        queries = self.split_proj(queries, batch_size)\n",
    "        keys = self.split_proj(keys, batch_size)\n",
    "        values = self.split_proj(values, batch_size)\n",
    "        \n",
    "        attention = scaled_dot_product_attention(queries, keys, values, mask)\n",
    "        \n",
    "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
    "        \n",
    "        concat_attention = tf.reshape(attention,\n",
    "                                      shape=(batch_size, -1, self.d_model))\n",
    "        \n",
    "        outputs = self.final_lin(concat_attention)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yiyuHe1OeT5N"
   },
   "source": [
    "## Codificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "UV0ZMH7KT_KZ"
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(layers.Layer):\n",
    "    \n",
    "    def __init__(self, FFN_units, nb_proj, dropout_rate):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.FFN_units = FFN_units\n",
    "        self.nb_proj = nb_proj\n",
    "        self.dropout_rate = dropout_rate\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.d_model = input_shape[-1]\n",
    "        \n",
    "        self.multi_head_attention = MultiHeadAttention(self.nb_proj)\n",
    "        self.dropout_1 = layers.Dropout(rate=self.dropout_rate)\n",
    "        self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        self.dense_1 = layers.Dense(units=self.FFN_units, activation=\"relu\")\n",
    "        self.dense_2 = layers.Dense(units=self.d_model)\n",
    "        self.dropout_2 = layers.Dropout(rate=self.dropout_rate)\n",
    "        self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "    def call(self, inputs, mask, training):\n",
    "        attention = self.multi_head_attention(inputs,\n",
    "                                              inputs,\n",
    "                                              inputs,\n",
    "                                              mask)\n",
    "        attention = self.dropout_1(attention, training=training)\n",
    "        attention = self.norm_1(attention + inputs)\n",
    "        \n",
    "        outputs = self.dense_1(attention)\n",
    "        outputs = self.dense_2(outputs)\n",
    "        outputs = self.dropout_2(outputs, training=training)\n",
    "        outputs = self.norm_2(outputs + attention)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "P-P92KeZih60"
   },
   "outputs": [],
   "source": [
    "class Encoder(layers.Layer):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 nb_layers,\n",
    "                 FFN_units,\n",
    "                 nb_proj,\n",
    "                 dropout_rate,\n",
    "                 vocab_size,\n",
    "                 d_model,\n",
    "                 name=\"encoder\"):\n",
    "        super(Encoder, self).__init__(name=name)\n",
    "        self.nb_layers = nb_layers\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.embedding = layers.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding()\n",
    "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
    "        self.enc_layers = [EncoderLayer(FFN_units,\n",
    "                                        nb_proj,\n",
    "                                        dropout_rate) \n",
    "                           for _ in range(nb_layers)]\n",
    "    \n",
    "    def call(self, inputs, mask, training):\n",
    "        outputs = self.embedding(inputs)\n",
    "        outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        outputs = self.pos_encoding(outputs)\n",
    "        outputs = self.dropout(outputs, training)\n",
    "        \n",
    "        for i in range(self.nb_layers):\n",
    "            outputs = self.enc_layers[i](outputs, mask, training)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7DthraBEwuvl"
   },
   "source": [
    "## Descodificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "7ZWZyFBnwy8u"
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(layers.Layer):\n",
    "    \n",
    "    def __init__(self, FFN_units, nb_proj, dropout_rate):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.FFN_units = FFN_units\n",
    "        self.nb_proj = nb_proj\n",
    "        self.dropout_rate = dropout_rate\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.d_model = input_shape[-1]\n",
    "        \n",
    "        # Self multi head attention\n",
    "        self.multi_head_attention_1 = MultiHeadAttention(self.nb_proj)\n",
    "        self.dropout_1 = layers.Dropout(rate=self.dropout_rate)\n",
    "        self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        # Multi head attention combinado con la salida del encoder \n",
    "        self.multi_head_attention_2 = MultiHeadAttention(self.nb_proj)\n",
    "        self.dropout_2 = layers.Dropout(rate=self.dropout_rate)\n",
    "        self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        # Feed foward\n",
    "        self.dense_1 = layers.Dense(units=self.FFN_units,\n",
    "                                    activation=\"relu\")\n",
    "        self.dense_2 = layers.Dense(units=self.d_model)\n",
    "        self.dropout_3 = layers.Dropout(rate=self.dropout_rate)\n",
    "        self.norm_3 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "    def call(self, inputs, enc_outputs, mask_1, mask_2, training):\n",
    "        attention = self.multi_head_attention_1(inputs,\n",
    "                                                inputs,\n",
    "                                                inputs,\n",
    "                                                mask_1)\n",
    "        attention = self.dropout_1(attention, training)\n",
    "        attention = self.norm_1(attention + inputs)\n",
    "        \n",
    "        attention_2 = self.multi_head_attention_2(attention,\n",
    "                                                  enc_outputs,\n",
    "                                                  enc_outputs,\n",
    "                                                  mask_2)\n",
    "        attention_2 = self.dropout_2(attention_2, training)\n",
    "        attention_2 = self.norm_2(attention_2 + attention)\n",
    "        \n",
    "        outputs = self.dense_1(attention_2)\n",
    "        outputs = self.dense_2(outputs)\n",
    "        outputs = self.dropout_3(outputs, training)\n",
    "        outputs = self.norm_3(outputs + attention_2)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "kpzdiWHiwywF"
   },
   "outputs": [],
   "source": [
    "class Decoder(layers.Layer):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 nb_layers,\n",
    "                 FFN_units,\n",
    "                 nb_proj,\n",
    "                 dropout_rate,\n",
    "                 vocab_size,\n",
    "                 d_model,\n",
    "                 name=\"decoder\"):\n",
    "        super(Decoder, self).__init__(name=name)\n",
    "        self.d_model = d_model\n",
    "        self.nb_layers = nb_layers\n",
    "        \n",
    "        self.embedding = layers.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding()\n",
    "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
    "        \n",
    "        self.dec_layers = [DecoderLayer(FFN_units,\n",
    "                                        nb_proj,\n",
    "                                        dropout_rate) \n",
    "                           for _ in range(nb_layers)]\n",
    "    \n",
    "    def call(self, inputs, enc_outputs, mask_1, mask_2, training):\n",
    "        outputs = self.embedding(inputs)\n",
    "        outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        outputs = self.pos_encoding(outputs)\n",
    "        outputs = self.dropout(outputs, training)\n",
    "        \n",
    "        for i in range(self.nb_layers):\n",
    "            outputs = self.dec_layers[i](outputs,\n",
    "                                         enc_outputs,\n",
    "                                         mask_1,\n",
    "                                         mask_2,\n",
    "                                         training)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x5sJYkjbz5DD"
   },
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "GqvqNjJPwyh-"
   },
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 vocab_size_enc,\n",
    "                 vocab_size_dec,\n",
    "                 d_model,\n",
    "                 nb_layers,\n",
    "                 FFN_units,\n",
    "                 nb_proj,\n",
    "                 dropout_rate,\n",
    "                 name=\"transformer\"):\n",
    "        super(Transformer, self).__init__(name=name)\n",
    "        \n",
    "        self.encoder = Encoder(nb_layers,\n",
    "                               FFN_units,\n",
    "                               nb_proj,\n",
    "                               dropout_rate,\n",
    "                               vocab_size_enc,\n",
    "                               d_model)\n",
    "        self.decoder = Decoder(nb_layers,\n",
    "                               FFN_units,\n",
    "                               nb_proj,\n",
    "                               dropout_rate,\n",
    "                               vocab_size_dec,\n",
    "                               d_model)\n",
    "        self.last_linear = layers.Dense(units=vocab_size_dec, name=\"lin_ouput\")\n",
    "    \n",
    "    def create_padding_mask(self, seq): #seq: (batch_size, seq_length)\n",
    "        mask = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "        return mask[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "    def create_look_ahead_mask(self, seq):\n",
    "        seq_len = tf.shape(seq)[1]\n",
    "        look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "        return look_ahead_mask\n",
    "    \n",
    "    def call(self, enc_inputs, dec_inputs, training):\n",
    "        enc_mask = self.create_padding_mask(enc_inputs)\n",
    "        dec_mask_1 = tf.maximum(\n",
    "            self.create_padding_mask(dec_inputs),\n",
    "            self.create_look_ahead_mask(dec_inputs)\n",
    "        )\n",
    "        dec_mask_2 = self.create_padding_mask(enc_inputs)\n",
    "        \n",
    "        enc_outputs = self.encoder(enc_inputs, enc_mask, training)\n",
    "        dec_outputs = self.decoder(dec_inputs,\n",
    "                                   enc_outputs,\n",
    "                                   dec_mask_1,\n",
    "                                   dec_mask_2,\n",
    "                                   training)\n",
    "        \n",
    "        outputs = self.last_linear(dec_outputs)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-c-LRThUPrso"
   },
   "source": [
    "# Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "qiOdqQ5qPs8z"
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Hiper Parámetros\n",
    "D_MODEL = 128 # 512\n",
    "NB_LAYERS = 4 # 6\n",
    "FFN_UNITS = 512 # 2048\n",
    "NB_PROJ = 8 # 8\n",
    "DROPOUT_RATE = 0.1 # 0.1\n",
    "\n",
    "transformer = Transformer(vocab_size_enc=VOCAB_SIZE_EN,\n",
    "                          vocab_size_dec=VOCAB_SIZE_ES,\n",
    "                          d_model=D_MODEL,\n",
    "                          nb_layers=NB_LAYERS,\n",
    "                          FFN_units=FFN_UNITS,\n",
    "                          nb_proj=NB_PROJ,\n",
    "                          dropout_rate=DROPOUT_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "46xg4Wrg1Wgl"
   },
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
    "                                                            reduction=\"none\")\n",
    "\n",
    "def loss_function(target, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(target, 0))\n",
    "    loss_ = loss_object(target, pred)\n",
    "    \n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    \n",
    "    return tf.reduce_mean(loss_)\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=\"train_accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "4Goque362343"
   },
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    \n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "        \n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps**-1.5)\n",
    "        \n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "leaning_rate = CustomSchedule(D_MODEL)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(leaning_rate,\n",
    "                                     beta_1=0.9,\n",
    "                                     beta_2=0.98,\n",
    "                                     epsilon=1e-9)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "Nb_32PIU5Zkh"
   },
   "outputs": [],
   "source": [
    "checkpoint_path = \"./drive/My Drive/Curso de NLP/Transformer/ckpt/\"\n",
    "\n",
    "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
    "                           optimizer=optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print(\"Último checkpoint restaurado!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "lhFK5kUx602K",
    "outputId": "bb4e7c4b-c112-478e-f76c-8ab979724455"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inicio del epoch 1\n",
      "Epoch 1 Lote 0 Pérdida 1.0137 Precisión 0.4548\n",
      "Epoch 1 Lote 50 Pérdida 1.1000 Precisión 0.4569\n",
      "Epoch 1 Lote 100 Pérdida 1.0996 Precisión 0.4608\n",
      "Epoch 1 Lote 150 Pérdida 1.0959 Precisión 0.4608\n",
      "Epoch 1 Lote 200 Pérdida 1.0908 Precisión 0.4602\n",
      "Epoch 1 Lote 250 Pérdida 1.0913 Precisión 0.4596\n",
      "Epoch 1 Lote 300 Pérdida 1.0878 Precisión 0.4603\n",
      "Epoch 1 Lote 350 Pérdida 1.0865 Precisión 0.4600\n",
      "Epoch 1 Lote 400 Pérdida 1.0848 Precisión 0.4608\n",
      "Epoch 1 Lote 450 Pérdida 1.0805 Precisión 0.4613\n",
      "Epoch 1 Lote 500 Pérdida 1.0767 Precisión 0.4613\n",
      "Epoch 1 Lote 550 Pérdida 1.0747 Precisión 0.4607\n",
      "Epoch 1 Lote 600 Pérdida 1.0723 Precisión 0.4603\n",
      "Epoch 1 Lote 650 Pérdida 1.0703 Precisión 0.4605\n",
      "Epoch 1 Lote 700 Pérdida 1.0691 Precisión 0.4607\n",
      "Epoch 1 Lote 750 Pérdida 1.0662 Precisión 0.4615\n",
      "Epoch 1 Lote 800 Pérdida 1.0621 Precisión 0.4623\n",
      "Epoch 1 Lote 850 Pérdida 1.0569 Precisión 0.4633\n",
      "Epoch 1 Lote 900 Pérdida 1.0527 Precisión 0.4641\n",
      "Epoch 1 Lote 950 Pérdida 1.0459 Precisión 0.4654\n",
      "Epoch 1 Lote 1000 Pérdida 1.0383 Precisión 0.4666\n",
      "Epoch 1 Lote 1050 Pérdida 1.0335 Precisión 0.4676\n",
      "Epoch 1 Lote 1100 Pérdida 1.0278 Precisión 0.4686\n",
      "Epoch 1 Lote 1150 Pérdida 1.0218 Precisión 0.4693\n",
      "Epoch 1 Lote 1200 Pérdida 1.0153 Precisión 0.4698\n",
      "Epoch 1 Lote 1250 Pérdida 1.0097 Precisión 0.4707\n",
      "Epoch 1 Lote 1300 Pérdida 1.0048 Precisión 0.4715\n",
      "Epoch 1 Lote 1350 Pérdida 0.9987 Precisión 0.4724\n",
      "Epoch 1 Lote 1400 Pérdida 0.9936 Precisión 0.4731\n",
      "Epoch 1 Lote 1450 Pérdida 0.9881 Precisión 0.4738\n",
      "Epoch 1 Lote 1500 Pérdida 0.9831 Precisión 0.4745\n",
      "Epoch 1 Lote 1550 Pérdida 0.9782 Precisión 0.4752\n",
      "Epoch 1 Lote 1600 Pérdida 0.9736 Precisión 0.4757\n",
      "Epoch 1 Lote 1650 Pérdida 0.9692 Precisión 0.4766\n",
      "Epoch 1 Lote 1700 Pérdida 0.9648 Precisión 0.4772\n",
      "Epoch 1 Lote 1750 Pérdida 0.9603 Precisión 0.4778\n",
      "Epoch 1 Lote 1800 Pérdida 0.9555 Precisión 0.4783\n",
      "Epoch 1 Lote 1850 Pérdida 0.9512 Precisión 0.4787\n",
      "Epoch 1 Lote 1900 Pérdida 0.9476 Precisión 0.4792\n",
      "Epoch 1 Lote 1950 Pérdida 0.9434 Precisión 0.4798\n",
      "Epoch 1 Lote 2000 Pérdida 0.9398 Precisión 0.4802\n",
      "Epoch 1 Lote 2050 Pérdida 0.9361 Precisión 0.4805\n",
      "Epoch 1 Lote 2100 Pérdida 0.9328 Precisión 0.4809\n",
      "Epoch 1 Lote 2150 Pérdida 0.9288 Precisión 0.4813\n",
      "Epoch 1 Lote 2200 Pérdida 0.9255 Precisión 0.4816\n",
      "Epoch 1 Lote 2250 Pérdida 0.9222 Precisión 0.4820\n",
      "Epoch 1 Lote 2300 Pérdida 0.9193 Precisión 0.4824\n",
      "Epoch 1 Lote 2350 Pérdida 0.9159 Precisión 0.4829\n",
      "Epoch 1 Lote 2400 Pérdida 0.9127 Precisión 0.4833\n",
      "Epoch 1 Lote 2450 Pérdida 0.9092 Precisión 0.4836\n",
      "Epoch 1 Lote 2500 Pérdida 0.9057 Precisión 0.4840\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 30\n",
    "for epoch in range(EPOCHS):\n",
    "    print(\"Inicio del epoch {}\".format(epoch+1))\n",
    "    start = time.time()\n",
    "    \n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    \n",
    "    for (batch, (enc_inputs, targets)) in enumerate(dataset):\n",
    "        dec_inputs = targets[:, :-1]\n",
    "        dec_outputs_real = targets[:, 1:]\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = transformer(enc_inputs, dec_inputs, True)\n",
    "            loss = loss_function(dec_outputs_real, predictions)\n",
    "        \n",
    "        gradients = tape.gradient(loss, transformer.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "        \n",
    "        train_loss(loss)\n",
    "        train_accuracy(dec_outputs_real, predictions)\n",
    "        \n",
    "        if batch % 50 == 0:\n",
    "            print(\"Epoch {} Lote {} Pérdida {:.4f} Precisión {:.4f}\".format(\n",
    "                epoch+1, batch, train_loss.result(), train_accuracy.result()))\n",
    "            \n",
    "    ckpt_save_path = ckpt_manager.save()\n",
    "    print(\"Guardando checkpoint para el epoch {} en {}\".format(epoch+1,\n",
    "                                                        ckpt_save_path))\n",
    "    print(\"Tiempo que ha tardado 1 epoch: {} segs\\n\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nmzyRwDrRGdq"
   },
   "source": [
    "# Evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "cNHwJJrz3lPB"
   },
   "outputs": [],
   "source": [
    "def evaluate(inp_sentence):\n",
    "    inp_sentence = \\\n",
    "        [VOCAB_SIZE_EN-2] + tokenizer_en.encode(inp_sentence) + [VOCAB_SIZE_EN-1]\n",
    "    enc_input = tf.expand_dims(inp_sentence, axis=0)\n",
    "    \n",
    "    output = tf.expand_dims([VOCAB_SIZE_ES-2], axis=0)\n",
    "    \n",
    "    for _ in range(MAX_LENGTH):\n",
    "        predictions = transformer(enc_input, output, False) #(1, seq_length, VOCAB_SIZE_ES)\n",
    "        \n",
    "        prediction = predictions[:, -1:, :]\n",
    "        \n",
    "        predicted_id = tf.cast(tf.argmax(prediction, axis=-1), tf.int32)\n",
    "        \n",
    "        if predicted_id == VOCAB_SIZE_ES-1:\n",
    "            return tf.squeeze(output, axis=0)\n",
    "        \n",
    "        output = tf.concat([output, predicted_id], axis=-1)\n",
    "        \n",
    "    return tf.squeeze(output, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "s6VeFKrE6Kdx"
   },
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "    output = evaluate(sentence).numpy()\n",
    "    \n",
    "    predicted_sentence = tokenizer_es.decode(\n",
    "        [i for i in output if i < VOCAB_SIZE_ES-2]\n",
    "    )\n",
    "    \n",
    "    print(\"Entrada: {}\".format(sentence))\n",
    "    print(\"Traducción predicha: {}\".format(predicted_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "id": "BupFjJlgDvCA",
    "outputId": "54e7aa51-1cc2-42a5-ede2-304d1a230d08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrada: This is a problem we have to solve.\n",
      "Traducción predicha: Es un problema que debemos solucionar.\n"
     ]
    }
   ],
   "source": [
    "translate(\"This is a problem we have to solve.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "id": "ZdoWKbCP7Czs",
    "outputId": "dbe5425f-1c91-45a7-87ab-4ed0dbf13d74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrada: This is a really powerful tool!\n",
      "Traducción predicha: Es una verdadera vía que nos parece muy poderosa.\n"
     ]
    }
   ],
   "source": [
    "translate(\"This is a really powerful tool!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "id": "UGjBEb5WFMGt",
    "outputId": "a7892348-eaf4-4445-d816-3c587275f436"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrada: This is an interesting course about Natural Language Processing\n",
      "Traducción predicha: Es un procedimiento de trabajo de naturaleza cultural\n"
     ]
    }
   ],
   "source": [
    "translate(\"This is an interesting course about Natural Language Processing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "L1v9g4T0FcO3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrada: Where is my pencil\n",
      "Traducción predicha: En lo que respecta a mi país\n"
     ]
    }
   ],
   "source": [
    "translate(\"Where is my pencil\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Transformer para NLP.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
